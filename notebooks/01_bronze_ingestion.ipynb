{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96302599-ed43-46b3-a4ca-958a006eacfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **Bronze Layer - Raw Data Ingestion**\n",
    "This notebook ingests the raw Online Retail II dataset from a managed volume into the Databricks Lakehouse.  \n",
    "It uses Pandas to read the Excel file, adds audit metadata, and saves the raw data as a Delta table (`bronze_sales`) for further processing in the Silver layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b39df952-efe9-4b05-98dc-978daf8566a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install openpyxl to enable reading Excel files with pandas\n",
    "%pip install openpyxl\n",
    "# Step 1: Importing pandas to read the Excel file\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: definition of the full path to the uploaded Excel file in your volume\n",
    "file_path = \"/Volumes/workspace/default/ecommerce_demo/online_retail_II.xlsx\"\n",
    "\n",
    "# Step 3: Read the Excel file using pandas\n",
    "pdf = pd.read_excel(file_path)\n",
    "\n",
    "# Step 4: Show the first 5 rows of data\n",
    "pdf.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e70236-c40f-486e-b2f9-093b45de6e02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Step 2: Conversion to Spark DataFrame\n",
    "\n",
    "The raw Excel file has been successfully loaded into a Pandas DataFrame (`pdf`).  \n",
    "It is now converted into a Spark DataFrame (`df_raw`) to enable distributed data processing using Apache Spark.  \n",
    "This transformation prepares the dataset for downstream ETL operations and storage in Delta Lake format as part of the Bronze layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "856a90a8-b79e-4219-bfb5-de43a795b07d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert all columns to string type to avoid mixed-type conversion issues\n",
    "pdf_clean = pdf.astype(str)\n",
    "\n",
    "# Now safely convert to Spark DataFrame\n",
    "df_raw = spark.createDataFrame(pdf_clean)\n",
    "\n",
    "# Preview the result\n",
    "df_raw.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5051ff7-081e-4671-8d85-0b1e78860a97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Add Ingestion Audit Column\n",
    "\n",
    "To support data traceability and lineage tracking, an additional column named `ingested_at` is appended to the dataset.  \n",
    "This column captures the exact timestamp at which the data was ingested into the Databricks environment.  \n",
    "Including such metadata is a standard practice in data engineering pipelines to aid in debugging, version control, and historical tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17df561b-8810-4b07-b3e5-45c3f4fdabec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Add an ingestion timestamp column to the Spark DataFrame\n",
    "df_bronze = df_raw.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "# Display the result with the audit column\n",
    "df_bronze.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31df9f3d-0ee2-438b-8662-692a3f6b2a43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ✏️ Step 3.1: Adjust Column Names for Delta Compatibility\n",
    "\n",
    "To comply with Delta Lake's technical constraints, column names containing spaces or special characters have been adjusted.  \n",
    "Specifically, spaces were replaced with underscores (e.g., `Customer ID` → `Customer_ID`). This ensures compatibility with Delta Lake's schema enforcement rules.\n",
    "\n",
    "It is important to note that no data values were altered during this process.  \n",
    "The dataset remains an accurate representation of the raw input, preserving the integrity of the Bronze layer while enabling reliable storage in Delta format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5603d805-4f3e-44a4-bf52-1db6cbf69c8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step: Clean column names by replacing spaces with underscores\n",
    "df_bronze = df_bronze.toDF(*[col.replace(\" \", \"_\") for col in df_bronze.columns])\n",
    "\n",
    "# Display to confirm updated column names\n",
    "df_bronze.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d6639d1-7088-49d5-8369-8249ac3ca3eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Save as Bronze Delta Table\n",
    "\n",
    "The Spark DataFrame, now appended with an `ingested_at` timestamp column, is written to persistent storage as a Delta Table named `bronze_sales`.  \n",
    "This marks the completion of the Bronze layer, where raw data is captured in its original form for auditability and reproducibility.  \n",
    "By using Delta Lake format, the dataset becomes capable of supporting version control, schema enforcement, and time travel features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4ccb704-2c34-4d26-9ab2-bc7e46a58f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the final DataFrame as a Delta Table (Bronze layer)\n",
    "df_bronze.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_sales\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
