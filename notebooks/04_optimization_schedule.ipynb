{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4ab7a10-f24c-4f24-943a-adce3cb36287",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook: 04_optimization_schedule\n",
    "\n",
    "## Purpose:\n",
    "This notebook serves as the **orchestration layer** of the data pipeline. It executes the full ETL flow, from raw data ingestion to aggregated insights, and applies performance optimizations post-processing.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Responsibilities:\n",
    "\n",
    "1. **Trigger Upstream Notebooks**\n",
    "   - Executes the Bronze, Silver, and Gold transformation notebooks in sequence using `dbutils.notebook.run`.\n",
    "\n",
    "2. **Optimize Delta Tables**\n",
    "   - Applies Delta Lake performance commands such as:\n",
    "     - `OPTIMIZE` to compact small files.\n",
    "     - `ZORDER BY` to boost query performance for specific fields.\n",
    "     - `VACUUM` to clean obsolete files and reduce storage cost.\n",
    "\n",
    "3. **Simulate Daily Production Run**\n",
    "   - Represents a real-world job scheduler like Databricks Workflows, Airflow, or CRON.\n",
    "   - Ensures that the pipeline can be repeatedly and reliably executed end-to-end.\n",
    "\n",
    "---\n",
    "\n",
    "##  Outcome:\n",
    "By running this notebook, a complete and optimized data pipeline is executedâ€”ensuring clean, curated, and performant datasets are available for downstream analytics or BI dashboards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d30480c6-61cb-42f6-8e2a-a19a1a1df72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Step 1: Load Bronze, Silver, and Gold Tables\n",
    "\n",
    "Each Delta table from the ETL pipeline is loaded to validate its existence and structure prior to optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9d62bd1-9f17-4b4a-acb8-1f4af7c513a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read.table(\"bronze_sales\")\n",
    "df_silver = spark.read.table(\"silver_sales\")\n",
    "df_gold = spark.read.table(\"gold_sales\")\n",
    "\n",
    "df_gold.display()  # Just previewing one layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b71dbcf-e16f-4ca8-960c-61cde2412993",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Optimize and ZORDER Tables\n",
    "\n",
    "The `OPTIMIZE` command is used to compact small files and improve read performance.  \n",
    "`ZORDER BY` further clusters files based on a filter column such as `InvoiceDate`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94fd1f25-ce07-4a40-b3af-d93572233203",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "OPTIMIZE gold_sales ZORDER BY (InvoiceDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42ce6417-cb05-49b4-9baf-380ebc04e8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  Step 3: Vacuum to Reclaim Storage\n",
    "\n",
    "To reduce storage costs, old data files not needed for time travel are removed using the `VACUUM` command.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbc2ab9-258c-4704-943c-35e084d9224d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SET spark.databricks.delta.retentionDurationCheck.enabled = false;\n",
    "\n",
    "VACUUM gold_sales RETAIN 168 HOURS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c68b355f-59be-48f9-8ef3-e05b6c943a4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Delta Table History and Time Travel\n",
    "\n",
    "Delta Lake maintains version history and allows rollback to previous states using version number or timestamp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b55784be-c914-49d9-b3d0-69f172e6ba0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY gold_sales;\n",
    "\n",
    "-- Example rollback preview\n",
    "SELECT * FROM gold_sales VERSION AS OF 0;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0139a974-f803-429e-be02-964c43e04168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5: Simulated Job Scheduling\n",
    "\n",
    "Each pipeline notebook can be chained programmatically using `%run` to simulate scheduled execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d7d967b-bd98-4162-b2fe-f697c6c17ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Simulate daily run\n",
    "dbutils.notebook.run(\"01_bronze_ingestion\", 300)\n",
    "dbutils.notebook.run(\"02_silver_cleaning\", 300)\n",
    "dbutils.notebook.run(\"03_gold_aggregation\", 300)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8641544026593142,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_optimization_schedule",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
